{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Load and visualize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "train_data = pd.read_csv('Train.csv')\n",
    "\n",
    "#Split data and labels\n",
    "X = train_data.iloc[:, 1:]  \n",
    "y = train_data.iloc[:, 0]\n",
    "\n",
    "#Process data\n",
    "if not isinstance(X, pd.DataFrame):\n",
    "    X = pd.DataFrame(X)\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "X = X.fillna(0)  \n",
    "X = X.values / 255.0\n",
    "X = X.reshape(-1, 28, 28, 1)\n",
    "\n",
    "#Visualize data as image on chosen index\n",
    "plt.imshow(X[1].reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "ACTIVATION FUNCTION \n",
    "\n",
    "\n",
    "USE ReLU Functions to introduce non-linearity for model can do multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.cache = None \n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        self.cache = X \n",
    "        # Luu nhung tham so duoc kich hoat de luc truyen GD thi chi truyen cho nhung tham so duoc kich hoat \n",
    "        return np.max(0,X)\n",
    "    def backward(self, dout: np.ndarray) -> np.ndarray:\n",
    "        X = self.cache\n",
    "        return dout * (X > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "SOFTMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.cache = None \n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        exp_X = np.exp(X -np.max(X, axis=1, keepdims=True))\n",
    "        self.cache = exp_X / np.sum(exp_X,axis =1, keepdims=True)\n",
    "        return self.cache\n",
    "    def backward(self, y: np.ndarray) -> np.ndarray:\n",
    "        return self.cache - y\n",
    "    def loss(self, y: np.ndarray) -> float:\n",
    "        probs = self.cache\n",
    "        #prevent log(0)\n",
    "        probs = np.clip(probs, 1e-15, 1.0)\n",
    "        n = probs.shape[0]\n",
    "        loss = -cp.sum(y * np.log(probs) / n)\n",
    "        return float(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "FULLY CONNECTED LAYER (FC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        self.W = np.random.randn(in_features, out_features) * np.sqrt(2.0 / in_features)\n",
    "        self.b = np.zeros((1, out_features))\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        self.cache = X\n",
    "        return X @ self.W + self.b\n",
    "    \n",
    "    def backward(self, dout: np.ndarray) -> np.ndarray:\n",
    "        X = self.cache\n",
    "        self.dW = X.T @ dout\n",
    "        self.db = cp.sum(dout, axis=0, keepdims=True)\n",
    "        return dout @ self.W.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "BACKPROP "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "OPTIMIZER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "LEARNING AND TRAINING "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "CONVOLUTION\n",
    "\n",
    "PARAMETERS:\n",
    "+ Image  : 3D ARRAY\n",
    "+ K      : Kernel (or filters) small matrix slide on images pixel \n",
    "+ padding: The ammount pixel \n",
    "+ stride : jump of the kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D:\n",
    "    \"\"\"Convolutional layer using im2col algorithm\"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        # He initialization\n",
    "        scale = np.sqrt(2.0 / (in_channels * kernel_size * kernel_size))\n",
    "        self.W = np.random.randn(out_channels, in_channels, kernel_size, kernel_size) * scale\n",
    "        self.b = np.zeros((out_channels, 1))\n",
    "        \n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        n, c, h, w = X.shape\n",
    "        \n",
    "        if self.padding > 0:\n",
    "            X_padded = np.pad(X, ((0, 0), (0, 0), (self.padding, self.padding), (self.padding, self.padding)), mode='constant')\n",
    "        else:\n",
    "            X_padded = X\n",
    "            \n",
    "        h_out = (h + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        w_out = (w + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        \n",
    "        # 1. Transform Image to Columns\n",
    "        X_col = self.im2col(X_padded, self.kernel_size, self.stride)\n",
    "        \n",
    "        # 2. Reshape Weights: (Out_C, In_C * K * K)\n",
    "        W_col = self.W.reshape(self.out_channels, -1)\n",
    "        \n",
    "        # 3. Matrix Multiplication: (Out_C, N * H_out * W_out)\n",
    "        out = W_col @ X_col + self.b\n",
    "        \n",
    "        # 4. Reshape back to image format: (Out_C, H_out, W_out, N) -> (N, Out_C, H_out, W_out)\n",
    "        out = out.reshape(self.out_channels, h_out, w_out, n)\n",
    "        out = out.transpose(3, 0, 1, 2)\n",
    "        \n",
    "        self.cache = (X, X_col)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout: np.ndarray) -> np.ndarray:\n",
    "        X, X_col = self.cache\n",
    "        n, c, h, w = X.shape\n",
    "        \n",
    "        # Prepare dout: (Out_C, N * H_out * W_out)\n",
    "        dout_reshaped = dout.transpose(1, 2, 3, 0).reshape(self.out_channels, -1)\n",
    "        \n",
    "        # Gradient w.r.t weights\n",
    "        self.dW = (dout_reshaped @ X_col.T).reshape(self.W.shape)\n",
    "        self.db = cp.sum(dout_reshaped, axis=1, keepdims=True)\n",
    "        \n",
    "        # Gradient w.r.t input column\n",
    "        W_col = self.W.reshape(self.out_channels, -1)\n",
    "        dX_col = W_col.T @ dout_reshaped\n",
    "        \n",
    "        # Column to Image\n",
    "        dX = self.col2im(dX_col, X.shape, self.kernel_size, self.stride, self.padding)\n",
    "        \n",
    "        return dX\n",
    "    \n",
    "    def im2col(self, X: np.ndarray, kernel_size: int, stride: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Transforms input image to column matrix.\n",
    "        Output shape: (C * K * K, N * H_out * W_out)\n",
    "        \"\"\"\n",
    "        n, c, h, w = X.shape\n",
    "        h_out = (h - kernel_size) // stride + 1\n",
    "        w_out = (w - kernel_size) // stride + 1\n",
    "        \n",
    "        # Index calculation\n",
    "        i0 = np.repeat(cp.arange(kernel_size), kernel_size)\n",
    "        i0 = np.tile(i0, c)\n",
    "        i1 = stride * np.repeat(np.arange(h_out), w_out)\n",
    "        \n",
    "        j0 = np.tile(np.arange(kernel_size), kernel_size * c)\n",
    "        j1 = stride * np.tile(np.arange(w_out), h_out)\n",
    "        \n",
    "        i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "        j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "        \n",
    "        k = np.repeat(np.arange(c), kernel_size * kernel_size).reshape(-1, 1)\n",
    "        \n",
    "        # Get columns: (C*K*K, H_out*W_out, N) -> (C*K*K, N*H_out*W_out)\n",
    "        # Using advanced indexing. This creates a large temporary array.\n",
    "        cols = X[:, k, i, j] \n",
    "        cols = cols.transpose(1, 2, 0).reshape(c * kernel_size * kernel_size, -1)\n",
    "        return cols\n",
    "    \n",
    "    def col2im(self, cols: np.ndarray, X_shape: Tuple, kernel_size: int, stride: int, padding: int) -> np.ndarray:\n",
    "        n, c, h, w = X_shape\n",
    "        h_padded, w_padded = h + 2 * padding, w + 2 * padding\n",
    "        X_padded = np.zeros((n, c, h_padded, w_padded))\n",
    "        \n",
    "        h_out = (h_padded - kernel_size) // stride + 1\n",
    "        w_out = (w_padded - kernel_size) // stride + 1\n",
    "        \n",
    "        i0 = np.repeat(np.arange(kernel_size), kernel_size)\n",
    "        i0 = np.tile(i0, c)\n",
    "        i1 = stride * np.repeat(np.arange(h_out), w_out)\n",
    "        j0 = np.tile(np.arange(kernel_size), kernel_size * c)\n",
    "        j1 = stride * np.tile(np.arange(w_out), h_out)\n",
    "        i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "        j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "        k = np.repeat(np.arange(c), kernel_size * kernel_size).reshape(-1, 1)\n",
    "        \n",
    "        cols_reshaped = cols.reshape(c * kernel_size * kernel_size, -1, n)\n",
    "        cols_reshaped = cols_reshaped.transpose(2, 0, 1)\n",
    "        \n",
    "        # Vectorized accumulation using add.at\n",
    "        # Slice(None) corresponds to 'n' dimension\n",
    "        np.add.at(X_padded, (slice(None), k, i, j), cols_reshaped)\n",
    "        \n",
    "        if padding > 0:\n",
    "            return X_padded[:, :, padding:-padding, padding:-padding]\n",
    "        return X_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "POOLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Max_Pooling(input_array, kernel_size=2, stride=1):\n",
    "    k, s = kernel_size, stride\n",
    "    \n",
    "    # Calculate output shape\n",
    "    output_shape = ((input_array.shape[0] - kernel_size) // stride) + 1\n",
    "    \n",
    "    # Initialize output array with correct shape\n",
    "    output = np.zeros((output_shape, output_shape))\n",
    "    \n",
    "    # Perform max pooling\n",
    "    for i in range(output_shape):\n",
    "        for j in range(output_shape):\n",
    "            # Extract the window\n",
    "            window = input_array[i*s:i*s + k, j*s:j*s + k]\n",
    "            # Get max value from window\n",
    "            output[i][j] = np.max(window)\n",
    "    \n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "FLATTENING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray\n",
    "        self.cache = X.shape\n",
    "        retrun X.reshape(X.shape[0], -1)\n",
    "    def backward(self, dout: np.ndarray) -> np.ndarray  \n",
    "        return dout.reshape(self.cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "SIGMOID AND TANH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "BACKPROB AGAIN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "MODEL LAYOUT\n",
    "- Input : images (resolution of 28 x 28 and monochromatic) of hand written digits\n",
    "- Block 1\n",
    "    + Conv2D (1)\n",
    "    + Activation (1)\n",
    "    + Max Pooling (1)\n",
    "- Block 2\n",
    "    + Conv2D (2)\n",
    "    + Activation (2)\n",
    "    + Max Pooling (2)\n",
    "- Flatten\n",
    "- Block 3\n",
    "    + Fully Connected (1)\n",
    "    + Activation (3)\n",
    "- Fully Connected\n",
    "- Softmax"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
